{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sammii0207/sami/blob/main/In_class_exercise_03_02282023_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sOIGF_Tz7K_"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L87Urqprz7LC"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwDni1p4z7LD"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWaIlk5Gz7LD"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I find sentiment analysis to be an interesting text classification task. It involves determining the emotional tone of a given piece of text, such as a tweet, product review, or news article. \n",
        "Sentiment analysis has many practical applications, such as monitoring public opinion, predicting customer behavior, or analyzing political discourse.\n",
        "\n",
        "To build a machine learning model for sentiment analysis, we need to extract relevant features from the text that can help us distinguish between positive, \n",
        "negative, and neutral sentiments. Here are five types of features that might be useful:\n",
        "\n",
        "\n",
        "Firstly, bag-of-words features capture the frequency of each word in the text, ignoring the order and context of the words. \n",
        "This type of feature can help the model identify the most frequent words associated with positive or negative sentiments, such as \"great,\" \"terrible,\" \"love,\" or \"hate.\"\n",
        "\n",
        "Secondly, part-of-speech (POS) features indicate the grammatical category of each word in the text, such as noun, verb, adjective, or adverb. \n",
        "This type of feature can help the model identify the syntactic patterns associated with different sentiments, such as the use of more adjectives in positive reviews.\n",
        "\n",
        "Thirdly, sentiment lexicons are lists of words annotated with their polarity, such as positive, negative, or neutral. \n",
        "This type of feature can help the model leverage external knowledge about the emotional connotations of words, even if they are rare or context-dependent.\n",
        "\n",
        "Fourthly, emotion features capture the presence and intensity of specific emotions in the text, such as joy, sadness, anger, or fear. \n",
        "This type of feature can help the model capture the nuances of different emotional states and distinguish between similar sentiments, such as frustration and anger.\n",
        "\n",
        "Lastly, stylistic features capture various aspects of the writing style, such as sentence length, punctuation, capitalization, or emoticons. \n",
        "This type of feature can help the model capture the subjective or ironic tone of the text and detect sarcasm or humor that might affect the sentiment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZioFqYC1z7LD"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngZH7TGsz7LE",
        "outputId": "9dac39e0-de4b-4721-9ed6-cf9743311743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Feature Extraction Results =====\n",
            "\n",
            "Bag-of-words:\n",
            "I: 1\n",
            "absolutely: 1\n",
            "loved: 1\n",
            "this: 1\n",
            "product: 1\n",
            "!: 1\n",
            "It: 1\n",
            "exceeded: 1\n",
            "my: 1\n",
            "expectations: 1\n",
            "in: 1\n",
            "every: 1\n",
            "way: 1\n",
            ".: 1\n",
            "\n",
            "POS tags:\n",
            "adverb: 1 - absolutely\n",
            "verb: 2 - loved, exceeded\n",
            "noun: 2 - product, way\n",
            "noun: 1 - expectations\n",
            "IN: 1\n",
            "\n",
            "Sentiment words:\n",
            "loved: 1\n",
            "\n",
            "Positive emotions:\n",
            "exceeded: 1\n",
            "\n",
            "Positive emotions:\n",
            "neg: 0.0\n",
            "\n",
            "Negative emotions:\n",
            "neu: 0.69\n",
            "\n",
            "Negative emotions:\n",
            "pos: 0.31\n",
            "\n",
            "Negative emotions:\n",
            "neg: 0.184\n",
            "neu: 0.816\n",
            "pos: 0.0\n",
            "\n",
            "Neutral emotions:\n",
            "neg: 0.135\n",
            "\n",
            "Stylistic features:\n",
            "Number of characters: 74\n",
            "Number of words: 14\n",
            "Number of sentences: 2\n",
            "Number of exclamation marks: 1\n",
            "Number of question marks: 0\n",
            "Number of capitalized words: 1\n",
            "Number of emoticons: 0\n",
            "neu: 0.565\n",
            "\n",
            "Stylistic features:\n",
            "Number of characters: 74\n",
            "Number of words: 14\n",
            "Number of sentences: 2\n",
            "Number of exclamation marks: 1\n",
            "Number of question marks: 0\n",
            "Number of capitalized words: 1\n",
            "Number of emoticons: 0\n",
            "pos: 0.3\n",
            "\n",
            "Stylistic features:\n",
            "Number of characters: 74\n",
            "Number of words: 14\n",
            "Number of sentences: 2\n",
            "Number of exclamation marks: 1\n",
            "Number of question marks: 0\n",
            "Number of capitalized words: 1\n",
            "Number of emoticons: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import opinion_lexicon\n",
        "\n",
        "# Sample texts for each type of emotion\n",
        "positive_text = \"I absolutely loved this product! It exceeded my expectations in every way.\"\n",
        "negative_text = \"I was very disappointed with this product. It did not live up to my expectations at all.\"\n",
        "neutral_text = \"This product is okay. It's not great, but it's not terrible either.\"\n",
        "\n",
        "# Bag-of-words feature extraction\n",
        "tokens = word_tokenize(positive_text)\n",
        "bag_of_words = nltk.FreqDist(tokens)\n",
        "\n",
        "# POS feature extraction\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "pos_counts = nltk.Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "# Sentiment lexicon feature extraction\n",
        "positive_words = set(opinion_lexicon.positive())\n",
        "negative_words = set(opinion_lexicon.negative())\n",
        "sentiment_words = [word for word in tokens if word in positive_words or word in negative_words]\n",
        "sentiment_counts = nltk.FreqDist(sentiment_words)\n",
        "\n",
        "# Emotion feature extraction\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "positive_scores = analyzer.polarity_scores(positive_text)\n",
        "positive_emotion_counts = {emotion: score for emotion, score in positive_scores.items() if emotion != \"compound\"}\n",
        "\n",
        "negative_scores = analyzer.polarity_scores(negative_text)\n",
        "negative_emotion_counts = {emotion: score for emotion, score in negative_scores.items() if emotion != \"compound\"}\n",
        "\n",
        "neutral_scores = analyzer.polarity_scores(neutral_text)\n",
        "neutral_emotion_counts = {emotion: score for emotion, score in neutral_scores.items() if emotion != \"compound\"}\n",
        "\n",
        "# Stylistic feature extraction\n",
        "num_chars = len(positive_text)\n",
        "num_words = len(tokens)\n",
        "num_sentences = len(nltk.sent_tokenize(positive_text))\n",
        "num_exclamation_marks = positive_text.count(\"!\")\n",
        "num_question_marks = positive_text.count(\"?\")\n",
        "num_capitalized_words = len([word for word in tokens if word.isupper()])\n",
        "num_emoticons = len(re.findall(r'[:;][-^]?[\\)DpP\\(\\[{\\|@}]', positive_text))\n",
        "\n",
        "#Print the extracted features\n",
        "print(\"\\n===== Feature Extraction Results =====\")\n",
        "print(\"\\nBag-of-words:\")\n",
        "for word, freq in bag_of_words.items():\n",
        "  print(f\"{word}: {freq}\")\n",
        "print(\"\\nPOS tags:\")\n",
        "for tag, count in pos_counts.items():\n",
        "  if tag.startswith('N'):\n",
        "    print(f\"noun: {count} - {', '.join(word for word, t in pos_tags if t == tag)}\")\n",
        "  elif tag.startswith('V'):\n",
        "    print(f\"verb: {count} - {', '.join(word for word, t in pos_tags if t == tag)}\")\n",
        "  elif tag.startswith('J'):\n",
        "    print(f\"adjective: {count} - {', '.join(word for word, t in pos_tags if t == tag)}\")\n",
        "  elif tag.startswith('R'):\n",
        "    print(f\"adverb: {count} - {', '.join(word for word, t in pos_tags if t == tag)}\")\n",
        "else:\n",
        "  print(f\"{tag}: {count}\")\n",
        "  print(\"\\nSentiment words:\")\n",
        "for word, freq in sentiment_counts.items():\n",
        "  print(f\"{word}: {freq}\")\n",
        "  print(\"\\nPositive emotions:\")\n",
        "for emotion, score in positive_emotion_counts.items():\n",
        "  print(f\"{emotion}: {score}\")\n",
        "  print(\"\\nNegative emotions:\")\n",
        "for emotion, score in negative_emotion_counts.items():\n",
        "  print(f\"{emotion}: {score}\")\n",
        "print(\"\\nNeutral emotions:\")\n",
        "for emotion, score in neutral_emotion_counts.items():\n",
        "  print(f\"{emotion}: {score}\")\n",
        "  print(\"\\nStylistic features:\")\n",
        "  print(f\"Number of characters: {num_chars}\")\n",
        "  print(f\"Number of words: {num_words}\")\n",
        "  print(f\"Number of sentences: {num_sentences}\")\n",
        "  print(f\"Number of exclamation marks: {num_exclamation_marks}\")\n",
        "  print(f\"Number of question marks: {num_question_marks}\")\n",
        "  print(f\"Number of capitalized words: {num_capitalized_words}\")\n",
        "  print(f\"Number of emoticons: {num_emoticons}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTDQsBzz7LE"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BEqhY5Gz7LE",
        "outputId": "0f4656c7-4106-450c-e06a-fd662e84e6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked feature names:\n",
            "sentiment_lexicon\n",
            "bag_of_words=not disappointed worst worst wasted\n",
            "emotion=angry\n",
            "emotion=sad\n",
            "pos_tags=RB VBN JJ NN NN\n",
            "bag_of_words=I absolutely loved\n",
            "bag_of_words=nice liked exceeded\n",
            "bag_of_words=this was great excellent\n",
            "emotion=happy\n",
            "emotion=neutral\n",
            "pos_tags=NN NN NN NN VB JJ\n",
            "pos_tags=NN NN NN VB JJ JJ\n",
            "pos_tags=NN VB JJ\n",
            "readability_score\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Define features and labels\n",
        "features_list = [\n",
        "    {'bag_of_words': 'I absolutely loved'},\n",
        "    {'bag_of_words': 'this was great excellent'},\n",
        "    {'bag_of_words': 'nice liked exceeded'},\n",
        "    {'bag_of_words': 'not disappointed worst worst wasted'},\n",
        "    {'pos_tags': 'NN NN NN VB JJ JJ'},\n",
        "    {'pos_tags': 'NN NN NN NN VB JJ'},\n",
        "    {'pos_tags': 'NN VB JJ'},\n",
        "    {'pos_tags': 'RB VBN JJ NN NN'},\n",
        "    {'sentiment_lexicon': 2.1, 'emotion': 'happy', 'readability_score': 7.8},\n",
        "    {'sentiment_lexicon': -1.5, 'emotion': 'sad', 'readability_score': 6.2},\n",
        "    {'sentiment_lexicon': 0.8, 'emotion': 'neutral', 'readability_score': 8.3},\n",
        "    {'sentiment_lexicon': -2.3, 'emotion': 'angry', 'readability_score': 5.9},\n",
        "]\n",
        "\n",
        "labels = ['positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative',\n",
        "         'positive', 'negative', 'positive', 'negative']\n",
        "\n",
        "# Transform the list of dictionaries into a numeric array\n",
        "vectorizer = DictVectorizer(sparse=False)\n",
        "features_matrix = vectorizer.fit_transform(features_list)\n",
        "\n",
        "# Use SelectKBest with f_classif to rank the features\n",
        "selector = SelectKBest(f_classif, k='all')\n",
        "selector.fit_transform(features_matrix, labels)\n",
        "\n",
        "# Get the feature scores and rank them in descending order\n",
        "scores = selector.scores_\n",
        "sorted_scores = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "ranked_features = [feature_names[i] for i in sorted_scores]\n",
        "\n",
        "# Print the ranked feature names\n",
        "print(\"Ranked feature names:\")\n",
        "for feature in ranked_features:\n",
        "    print(feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ5Tkbrez7LE"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this before executing the below code\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I_ROs-HQicp",
        "outputId": "9c919d9d-9ff8-4c3f-e7d0-5344ba29b372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.26.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=8b7889308539d9f50c5605dfa7b2ebf624afda67044b48282dde23e7480e55fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0TCn2gMz7LE",
        "outputId": "843e83a5-6f11-4833-b9d0-0b8c9595e51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative text: 0.7740368843078613\n",
            "Neutral text: 0.6637380123138428\n",
            "Positive text: 0.21653777360916138\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the text data\n",
        "positive_text = \"I absolutely loved this product! It exceeded my expectations in every way.\"\n",
        "negative_text = \"I was very disappointed with this product. It did not live up to my expectations at all.\"\n",
        "neutral_text = \"This product is okay. It's not great, but it's not terrible either.\"\n",
        "\n",
        "# Define the query\n",
        "query = \"product did not exceeded expectations\"\n",
        "\n",
        "# Load the BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Tokenize the text data and query\n",
        "positive_tokens = model.tokenize(positive_text)\n",
        "negative_tokens = model.tokenize(negative_text)\n",
        "neutral_tokens = model.tokenize(neutral_text)\n",
        "query_tokens = model.tokenize(query)\n",
        "\n",
        "# Get the BERT embeddings for the text data and query\n",
        "positive_embeddings = model.encode([positive_text]).reshape(1,-1)\n",
        "negative_embeddings = model.encode([negative_text]).reshape(1,-1)\n",
        "neutral_embeddings = model.encode([neutral_text]).reshape(1,-1)\n",
        "query_embedding = model.encode([query]).reshape(1,-1)\n",
        "\n",
        "# Calculate the cosine similarity between the query and text embeddings\n",
        "positive_similarity = cosine_similarity(query_embedding, positive_embeddings)[0][0]\n",
        "negative_similarity = cosine_similarity(query_embedding, negative_embeddings)[0][0]\n",
        "neutral_similarity = cosine_similarity(query_embedding, neutral_embeddings)[0][0]\n",
        "\n",
        "# Print the similarity scores in descending order\n",
        "similarity_scores = {'Positive': positive_similarity, 'Negative': negative_similarity, 'Neutral': neutral_similarity}\n",
        "sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "for label, score in sorted_scores:\n",
        "    print(f'{label} text: {score}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}